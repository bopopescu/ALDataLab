/////////////////////////////////////////////////////////////////////////////
//pour spark-shell
/////////////////////////////////////////////////////////////////////////////

sqlContext.setConf("spark.sql.shuffle.partitions", "10")
import dlenv._
import dlutil._
import dlpipeline._
import DLRepo._

val D_Root="s3://alstomlezoomerus"

val pipe=new dlpipeline(D_Root+"/DATA/Repository")
val repo = new dlrepo(D_Root+"/DATA/Repository")


repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/MDM-ITC_20151117.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/I-ID_20151101.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/AIP-Application_20151117.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/AIP-Server_20151117.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/AIP-SoftInstance_20151117.csv")
repo.genAIP(sqlContext)

val dfMDM=repo.readMDM(sqlContext)

val dfAIPServer = repo.readAIPServer(sqlContext)
val dfAIPSoftInstance = repo.readAIPSoftInstance(sqlContext)
val dfAIPApplication = repo.readAIPApplication(sqlContext)

val dfAIP = repo.readAIP(sqlContext)
val ipdup = dfAIP.groupBy("aip_server_ip").agg(count("aip_server_ip") as "count_ip").filter($"count_ip" > 1).select("aip_server_ip").withColumnRenamed("aip_server_ip", "ip")

val df = dfAIP.join(ipdup, dfAIP("aip_server_ip") <=> ipdup("ip"), "inner")

df.sort(asc("aip_server_ip"))


pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/connection_sabad11478.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/webrequest_sabad11478.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/connection_sabad11479.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/webrequest_sabad11479.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline3to4(sqlContext, D_Root + "/DATA/3-NXFile/connection_sabad11478.ad.sys_2015-10-28.parquet", D_Root + "/DATA/4-NXFile")
pipe.pipeline3to4(sqlContext, D_Root + "/DATA/3-NXFile/connection_sabad11478.ad.sys_2015-10-27.parquet", D_Root + "/DATA/4-NXFile")


spark-shell --master yarn --driver-memory 4G --executor-memory 8G --jars ./lib/ALDataLab-assembly-1.0.jar

/////////////////////////////////////////////////////////////////////////////
Commandes spark-submit
/////////////////////////////////////////////////////////////////////////////

spark-submit --master yarn-cluster --driver-memory 2G --executor-memory 10G --class DLMain.DLMain file:///home/hadoop/lib/ALDataLab-assembly-1.0.jar --D_REPO s3://alstomlezoomerus/DATA/Repository --method pipeline3to4 s3://alstomlezoomerus/DATA/3-NXFile/connection_sabad11478.ad.sys_2015-10-30.parquet s3://alstomlezoomerus/DATA/4-NXFile



/////////////////////////////////////////////////////////////////////////////
Commandes shell (appel√©es en cron)
/////////////////////////////////////////////////////////////////////////////
/home/hadoop/script/ALDataLab-client.sh "s3://alstomlezoomerus/DATA/Repository" "s3://alstomlezoomerus/DATA/Repository/in/" "csv" "s3" ".todo" "RepoProcessInFile"
/home/hadoop/script/ALDataLab-client.sh "s3://alstomlezoomerus/DATA/Repository" "s3://alstomlezoomerus/DATA/2-NXFile" "sabad11478" "s3" ".todo" "pipeline2to3" "s3://alstomlezoomerus/DATA/3-NXFile"
/home/hadoop/script/ALDataLab-client.sh "s3://alstomlezoomerus/DATA/Repository" "s3://alstomlezoomerus/DATA/3-NXFile" "sabad11478" "s3" ".todo" "pipeline3to4" "s3://alstomlezoomerus/DATA/4-NXFile"

#uniq engine list to compute
enginelist = $(aws s3 ls s3://alstomlezoomerus/DATA/2-NXFile/ | awk '{print $4}' | grep -v "^$" | grep -v todo | awk -F'_' '{print $2}' | sort | uniq | wc -l)