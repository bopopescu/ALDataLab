/////////////////////////////////////////////////////////////////////////////
//pour spark-shell
/////////////////////////////////////////////////////////////////////////////

spark-shell --master yarn --driver-memory 2G --executor-memory 8G --jars ./lib/ALDataLab-assembly-1.0.jar


sqlContext.setConf("spark.sql.shuffle.partitions", "10")
import dlenv._
import dlutil._
import dlpipeline._
import DLRepo._

val D_Root="s3://alstomlezoomerus"

val pipe=new dlpipeline(D_Root+"/DATA/Repository")
val repo = new dlrepo(D_Root+"/DATA/Repository")


repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/MDM-ITC_20151117.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/I-ID_20151101.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/AIP-Application_20151117.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/AIP-Server_20151117.csv")
repo.ProcessInFile(sqlContext, D_Root + "/DATA/Repository/in/AIP-SoftInstance_20151117.csv")
repo.genAIP(sqlContext)

val dfMDM=repo.readMDM(sqlContext)

val dfAIPServer = repo.readAIPServer(sqlContext)
val dfAIPSoftInstance = repo.readAIPSoftInstance(sqlContext)
val dfAIPApplication = repo.readAIPApplication(sqlContext)

val dfAIP = repo.readAIP(sqlContext)
val ipdup = dfAIP.groupBy("aip_server_ip").agg(count("aip_server_ip") as "count_ip").filter($"count_ip" > 1).select("aip_server_ip").withColumnRenamed("aip_server_ip", "ip")

val df = dfAIP.join(ipdup, dfAIP("aip_server_ip") <=> ipdup("ip"), "inner")

df.sort(asc("aip_server_ip"))


pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/connection_sabad11478.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/webrequest_sabad11478.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/connection_sabad11479.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline2to3(sc, sqlContext, D_Root + "/DATA/2-NXFile/webrequest_sabad11479.ad.sys_20151031.tgz.csv.gz", D_Root + "/DATA/3-NXFile")
pipe.pipeline3to4(sc, sqlContext, D_Root + "/DATA/3-NXFile/connection_sabad15034.ad.sys_2015-10-01.parquet", D_Root + "/DATATEST/5-NXFile", D_Root + "/DATATEST/4-NXFile")



/////////////////////////////////////////////////////////////////////////////
Commandes spark-submit
/////////////////////////////////////////////////////////////////////////////

spark-submit --master yarn --driver-memory 2G --executor-memory 8G --class DLMain.DLMain file:///home/hadoop/lib/ALDataLab-assembly-1.0.jar --D_REPO s3://alstomlezoomerus/DATA/Repository --method pipeline3to4 s3://alstomlezoomerus/DATA/3-NXFile/connection_sabad11478.ad.sys_2015-10-30.parquet s3://alstomlezoomerus/DATA/5-NXFile s3://alstomlezoomerus/DATA/4-NXFile



/////////////////////////////////////////////////////////////////////////////
Commandes shell (appel√©es en cron)
/////////////////////////////////////////////////////////////////////////////

~/script/distributeByBatch.sh 7 20 "s3://alstomlezoomerus/DATA/2-NXFile/.*201511.*\.gz.todo$" ~/script/ALDataLab-client.sh s3://alstomlezoomerus/DATA/Repository pipeline2to3 s3://alstomlezoomerus/DATA/3-NXFile s3://alstomlezoomerus/DATA/2-control

~/script/ALDataLab-client.sh 1 s3://alstomlezoomerus/DATA/Repository RepoProcessInFile s3://alstomlezoomerus/DATA/Repository s3://alstomlezoomerus/DATA/Repository/control s3://alstomlezoomerus/DATA/Repository/in/MDM-ITC_20151117.csv s3://alstomlezoomerus/DATA/Repository/in/AIP-Application_20151117.csv s3://alstomlezoomerus/DATA/Repository/in/AIP-Server_20151117.csv s3://alstomlezoomerus/DATA/Repository/in/AIP-SoftInstance_20151117.csv s3://alstomlezoomerus/DATA/Repository/in/I-ID_20151207.csv

nohup spark-submit --master yarn --driver-memory 4G --executor-memory 5G --executor-cores 2 --num-executors 30 --class com.alstom.datalab.Main file:///home/hadoop/lib/ALDataLab-assembly-1.1.jar --method pipeline3to4 --dirin s3://alstomlezoomerus/DATA/3-NXFile --control s3://alstomlezoomerus/DATA/3-control --direrr s3://alstomlezoomerus/DATA/3-err --dirout s3://alstomlezoomerus/DATA/4-NXFile --repo s3://alstomlezoomerus/DATA/Repository s3://alstomlezoomerus/DATA/2-control/1449687290.csv&

spark-submit --master yarn --driver-memory 1G --executor-memory 8G --executor-cores 3 --class com.alstom.datalab.Main file:///home/hadoop/lib/ALDataLab-assembly-1.1.jar --method RepoProcessInFile --dirin s3://alstomlezoomerus/DATA/Repository/in --control s3://alstomlezoomerus/DATA/Repository/control --direrr s3://alstomlezoomerus/DATA/Repository/err --dirout s3://alstomlezoomerus/DATA/Repository --repo s3://alstomlezoomerus/DATA/Repository s3://alstomlezoomerus/DATA/Repository/in/MDM-ITC_20151117.csv


#divers shell
function lstpipegap {
    DIRROOT="s3://alstomlezoomerus"
    DIRIN="$DIRROOT/DATA/2-NXFile/"
    DIRCONTROL="$DIRROOT/DATA/2-control/"
    DIROUT="$DIRROOT/DATA/3-NXFile/"


    filein=$(aws s3 ls $DIRIN | grep -v folder | grep -v todo | grep -v done | grep -v going | awk '{if ($1 == "PRE") {print $2} else {print $4}}' | sed "s/\/$//" )
    filetodo=$(aws s3 ls $DIRIN | grep -v folder | grep todo| awk '{if ($1 == "PRE") {print $2} else {print $4}}' | sed "s/\.todo\/$//" | sed "s/\.todo$//")
    fileongoing=$(aws s3 ls $DIRIN | grep -v folder | grep ongoing | awk '{if ($1 == "PRE") {print $2} else {print $4}}' | sed "s/\.ongoing\/$//" | sed "s/\.ongoing$//")
    filedone=$(aws s3 ls $DIRIN | grep -v folder | grep done | awk '{if ($1 == "PRE") {print $2} else {print $4}}' | sed "s/\.done\/$//" | sed "s/\.done$//")
    filecontrol=$(aws s3 ls $DIRCONTROL | grep -v folder | awk '{if ($1 == "PRE") {print $2} else {print $4}}' | sed "s/\/$//")

    ResFileDone=""
    ResFileNotTodo=""
    for file in $filein
    do
        if echo $filetodo | grep -q $file; then
            ResFileTodo="$ResFileNotTodo $file"
        else
            echo "$file done"
            ResFileDone="$ResFileDone $file"
        fi
    done

}

