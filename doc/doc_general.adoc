



== Source collect

* Who
Who does collect these information ?
* What
What kind of information we collect ?
What are the source of our collected information ?
* Why
What is the purpose of this collection ?
What we want to achieve by collecting these information ?
* When
How often these information are collected ?
How many times per day/week/month ?
* Where
From where we collect those information ?
* How
How we collect these information ?
* How Much
What is the quantity of the collected information ?

What we collect as information to analyse ?

There are different kind of sources that we collect to analyse.

Main sources are :

* oracle database logs (demande is done as one shot)
* nexthink
* server sockets
* server usages


== References

References are

* AIP
* IDM
* MDM-ITC
* Storage Master Report (1/month)

=== Oracle log files

* Who
* What
* Why
* When
* Where
* How
* How Much

This is done as one shot, asked one time and thats all.

=== Nexthink

* Who
Who does do that ?
* What
What is nexthink ?
* Why
Why we do that ?
Why we do it by this way ?
* When
How often we do this ? Per month ? Per year ? Per hour ?
* Where
Where do we store these information, before and later ?
* How
How we do all these ?
* How Much
What is the quantity of the information ? etc...

This is the collection of the information related to devices such as PCs, Laptops, etc.. and servers.
Nexthink is the editor of the application which collect information about any actions of the user.
As this is only available for windows machines and servers, this kind of collect is only applied to windows based systems.


In fact, there are two kind of ngex? server which are in version 5 and 6.
And the collection of data is done separately on each servers.
There are about 21 server in 4, and 3 server in 6.

When we collect them as in level 0, they are not to clean to analyse.
This is why we create 2 VM on exact version of these machines.
They are VM5, VM6.

Collected data is deployed to these VMs and then we extract information that we need in CSV format.
Language used to extract information from collected data is NXQL.
And then we store them under level 1.

Later, we make device information, etc. anonymous on VM20.
And then we store them under s3 server.
(Precise location under s3 ? s3://collect/nexthink/in ?)




==== Nexthink in the project
Project source is under ALDatalab/collect/nexthink.


=== Kind of information stored

* Who
* What
* Why
* When
* Where
* How
* How Much

* connection

These are mainly requests done by the source devices.
Any HTTP requests, either in TCP or UDP protocol, are collected and stored.


* webrequest

These are only information about the sever DNS requested. (google, any web-site, etc.)

* execution

These are information about the applications executed on the user's machine which do not probably use internet requests.

=== Server sockets

* Who
* What
* Why
* When
* Where
* How
* How Much

This part is dedicated to collect 80% of the linux servers.
Each 5 minutes we do netstat + ps to get information about what is accessed and what are the processes related to these usage.

A pair public/private key generated and public keys are deployed to about on thousand linux servers.



We have deployed a public key on all thousand linux server to realise connection via ssh protocol.
Then we deployed our script which collect these information.

Then we access to these servers to get collected data, then we merge multiple files and store them under ....?(NAS)

==== Server socket in the project
Project source is under ALDatalab/collect/serversocket



=== Server usage

* Who
* What
* Why
* When
* Where
* How
* How Much


Server usage is mainly related to the information about the CPU, memory usage and storage usage.
We do 1 collect every 30 minute / or 1 hour.






Les log oracle, on les collecte les log oracle qui sont disponbile en instantanÃ©, mais on ne les ai pas en continue. 
This is onshot. 
They give us what they have but not au fil de l'eau. 


== References

=== AIP

refernciel des application d'infrastructure


=== IDM

referenciel de l'identify manager. 
We have list of all users, on which "site" they belongs, what's their login, what is their sector, their teams, etc. 


=== MDM-ITC
This is network topologie. 
As an example, for a given site, we know what are the range IP with these information. 


=== Storage Master Report
Which is updated once a month. 
This is a billing file/report for CSC which concern server memory usage.
* tous les stockage que l'on vous facture, 
* for each server and for each its instances, 
* what kind of storage we use, 
* totat amound of space allocated, 
* how much space is used
* and if it is already **factured**. 


These are list of all kind of sources to collect as information. 




Colllector server (which is name as SARMA10012) collect all usage information. 
These are "nexthink", "server usage", "server socket", "oracle log". 
The collecting process is different for each of them. 

The main object is to collect all information and put them on the local SAN server.
Later, we use VM20 to anonimize these information. 
And anonymous data is saved under SAN server. 
Finally it is sent to s3 server. 


This collocter server collect also IDM data which must be anonymized before sending to s3. 



Collecter use IDM data to anonymize user information. 
It creates the anonymized IDM which is named as I-ID. 
User secters, user teams, user sites are anonymized.
This is because we could later be able to know what is the sector, team, site of an anonymized user. 


A dictionnary file (dictionnary.csv) is created under SAN server to keep information about which user data correspond to which I-ID.
The collector server is the one which can tell us which anonymized user matches with its real information. 





Nexthink, server usage and server socket are then used inside of the process named "Pipeline". 

Pipeline is the execution of various batch. 
Pipeline is executed on cluster. 
This cluster will take whatever is there under s3. 
Files are encoded in "parquet" format. 

"Encoded process" => CSV file formats are encoded in "parquet" format with more column of information. 
"Resolved process" => Resolution of encoded data with different kind of referencial information. 
As an example, for a given user with match it with its sector, site and team. 

(This "Resolve" process is also done for "oracle logs" but it is done on Notebook)
(It is done inside of Pipeline for these data.)

(
In Oracle-Pipeline notebook
we parse all files and encode them on "parquet" format. 
Then we do resolve.
Finally on Oracle-Analyse notebook we apply "aggregation" process for oracle logs. 
)

Pipeline does these actions for those 3 data sources.
With aggregation, 
we will find 
* the number of connections, 
* the number of distinct users, 
* the number of distinct devices,
* the network traffic volume, 
(apply group by on several columns)
Another goal is to get reduced size of Dataset in order to make them easily analysable with Zeppelin. 

However, we can start from each step to continue these processes as we want on Zeppelin. 
It is possible to start and continue from any steps on Zeppelin. 
Data generated by Pipeline is stored under s3 server. 


"Oracle logs" are not concerned with this "Pipeline" process.
But we do exactly the same processes for oracle log files.

Whatever is prepared as data by Pipeline is stored on s3://gedatalab/in
Each time that we start pipeline, it uses data which is under s3://gedatalab/in.



Pipeline source code is under _/src/main/scala/_ folder in project. 










- For this documentation

* Put more information as we can to make the subject clear enough. 
* Make it readable, visually clear. 
* Use different colors to show how different kind of collect data sources are used in various cases. 
* Where the data come from, from where they pass and where it stored. 
* If it is red then it is used by Zeppelin, if it is blue then it is used by pipeline. 







== Nexthink

How it works. 

Data collected from PC computers, laptops, etc.. are stored in a 21 server
And whatever collected from servers are saved under 3 nexthink servers. 

There is a _Collector_ of Nexthink which is installed on users' machines.
It captures network connections, program executions, web request, etc.. and sends data to the Nexthink _Engine_.
Nexthink Engines stock collected data and make a backup of them daily per server. 

There are 21 collection of servers which are dedicated to store data coming from user's machines.
The version of Nexthink Engine installed on these servers is V6.

There are also 3 servers which collect Nexthink data from servers.
The version of Nexthink Engine installed on these servers is V5.

As a result, there are 24 backup daily. 

== Data transfer from backup servers

Every 3 days, collector server (SARMA10012) collect backup data from Nexthink Engine servers.
Each backup data can contain log history of 5-20 days. 

If there are more data in the backup servers, the old once are purged. 
This is why we need to be sure to take more data possible and not to loose a part of them, 
we copy them every 3 days to SAN server. 
As a result, we will have overlaps.
We will discuss about this problem in the Pipeline part. 

There are 2 virtual machines on the collector server which are dedicated to extract information from "BackupData".
One of them is VM5 with version 5 of Nexthink and it is used extract data from backup data. 
Another one is VM6 with version 6 of Nexthink and this one is used to extract backup data coming from 21 Nexthink servers. 

Pure data collected are saved under /0 folder on SAN server as they are.
Every day, the action _extractbackup_ is executed.
In other terms, the backup data saved under /0 folder is dispatched either on VM5 or on VM6 to be extracted. 


This extraction is done daily and data are stored under /1 folder in CSV format. 
As an information, NXQL is used as SQL language to extract data from Nexthink Engine database. 

Once done, we have 3 kind of files under /1 folder. 

* anything related to a "connection" (TCP, UDP, etc..)
Information collected about any outgoing (and only outgoing) network requests, such as which user is connected, by which application, IP address requested, HTTP protocol used, server port number, request execution time, request content size. 
These information are mainly related to the source machine of request, which can be a simple user machine but also a server. 

* anything related to a "webrequest" (DNS information)
This kind of data is captured while a web request (HTTP) is detected. 
Any information about the target machine is collected.
Full URL of a web request is not registered but only its DNS information.

* anything related to an "execution"
These are information about the execution of any application started by a user.
And this information is collected even if a user doesn't login to the application executed. 
Because there are some applications which don't need to ask any connection/login. 
In other terms, this give us information about :

* which user executed an application ?
* which application is executed ?
* at which time ?
* what is the version of the executed application ?
* how much does it take to be started
* what is the path to the application
* etc.


This why we have 3 different folder under s3://gedatalab/in which are _/connection_, _webrequest_, _execution_. 

[TIP]
give a picture from cyberdock with highlighted colors of these folders. 


The next action after extracting backup data and storing them on /1 folder, is to anonymize them. 
This process is also done on VM20. 
Anonymized data is stored under /2 folder on the SAN server in CSV format. 
Finally, it is also copied to s3://gedatalab/in. 





== PIPELINE




== PIPELINE wrap

What is pipeline ?
Why ?


* Who
Who do that ?
* What
What is pipeline ?
* Why
Why this pipeline ?
* When
When it is used and when it is done ?
* Where
Where are stored pipeline source code ? Where are stored information after each step ?
The input data is stored on s3 under s3://gedatalab/
* How
How it is done ? By which way ?



Pipeline is executed on cluster.
The input data is taken from s3://gedatalab/ and copied to local server under HDFS file system, and finally stored back on s3 server.


[TIP]
give a screenshot of what is in the cyberduck for s3://gedatalab


* /gedatalab/in - folder contains newly stored data which come from collector server.
* /gedatalab/pipeline - is the main folder to keep all kind of data used during different steps of pipeline actions.
Final result data are stored in pipeline. 

* pipeline/in folder will be used to keep default input data.
* pipeline/done folder is used to keep output data.
* pipeline/meta is dedicated to store meta information about _nexthink_.
* pipeline/metasocket folder is used to store server socket meta information.
* pipeline/repo folder is used to achieve to "resolve"

There are 3 main folders which are _encoded_, _resolved_, _aggregated_. 
This will help us to use each stay of data as we want.
There are sub-folders under each of them for each kind of data (nexthink, server usage, server socket).


Once we finished data manipulation we store them again on the s3 server.
* How Much
What is the quantity of the information used ?
How much can it cost us ? etc ?


Pipeline actions are
* encode data in CSV format
* resolve
* aggregate
 - Aggregate is to join tables with reference tables and if needed to groupby.



Firstly, pipeline makes an archive of the last version of s3/gedatalab/pipeline.
Then, whatever it is under /in folder is moved to /pipeline/in folder under s3.
We copy all data files from s3 pipeline to local hdfs://data/ under file system.
One all actions are done, output is stored back to s3://gedatalabe/pipeline.
Previous version, of course, is erased.

