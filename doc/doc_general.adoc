= DataLab
Guillaume PINOT, Matin BAYRAMOV
bayramov.matin-ext@power.alstom.com
v1.0, 2016-10-10




== References

In this project we have 3 main types of referenced data collection.
These are _AIP_, _MDM-ITC_ and _Storage Master Report_ which are kept on s3.

image::images/s3_references.jpg[title="Reference storage on s3", width="450", height="250"]

=== AIP

_AIP_ is data referential for all applications installed on different servers and used by different users.
In other words, AIP is a referential of the IT assets of Alstom and it covers mainly the list of applications,
the list of servers where the applications are deployed, the product software used by these applications,
the licenses used by the applications, etc.

There are 3 main tables which are :

* _server_: list of servers
* _application_: list of business applications
* _softInstance_: which application is deployed on which server

Data are synchronized daily by batches and they are stored under s3://gerepo/.

Zeppelin notebook URLs are

* https://devzeppelin.gadatalab.com/#/notebook/2BMCK757N
* https://devzeppelin.gadatalab.com/#/notebook/2BXZ39CTF


=== MDM-ITC

This is the network topology which gives us any information about network elements. 
As an example, we are able to know what are the _IP ranges_ for a given _site_ thanks to these information.
Today, these data are manually transferred into s3://gerepo/in/mdm-itc.


include::doc_storagemaster.adoc[]


NOTE: _IDM_ is the management of individual identities/users, their authentication, authorization,
roles and privileges within different sites, sectors, teams etc.

=== IDM

_IDM_ is used to resolve user information which are made anonymous before being stored on s3.
However, we don't need IDM to anonymize data.

// Une autre application qui gere tous les donnes des utilsateuur et leurs connnexion, etc.

include::doc_introduction.adoc[]


<<<
== Global view of main processes

Before presenting one by one of these collections and their usage analysis, here is an image
which shows common processes which are applied during various steps.

image::images/collect_pipeline.jpg[title="Global view of the common processes", width="650", height="350"]


Generally, the input data arrive to SAN server.
They are extracted into correct formats and made anonymous if necessary and stored on s3.
When needed they are copied to HDFS and analysed on the pipeline processes.
And IDM are used to resolve data if they are made anonymous.
In the following chapters we'll give some more detailed information about all of these items and processes.


Collector server (which is named as SARMA10012) collects different kind of data sources
which are _nexthink_, _server-usage_, _server-socket_, _oracle-log_.
While the way we collect them varies, common actions are performed
and same environments are used to process and analyse them.

The main object is to collect all information and put them on the local SAN server.
Later, we use VM20 to anonymize these information.
And anonymous data are saved under SAN server.
Finally it is sent to s3 server.


Collector server have access to IDM data which must be anonymized before sending to s3.


It creates the anonymized IDM which is named as I-ID.
This is because we could later be able to know what is the sector, team, site of an anonymized user.


A dictionary file (dictionary.csv) is created under SAN server to keep information about which user data correspond to which I-ID.
The collector server is the one which can tell us which anonymized user matches with its real information.


Nexthink, server usage and server socket are then used inside of the process named "Pipeline".


include::doc_nexthink.adoc[]


include::doc_serversocket.adoc[]


include::doc_serverusage.adoc[]


include::doc_pipeline.adoc[]


// It is important to know that there is
// Apart from +ing, ..... may also be required
// To know more about ....., look into/check out
// However, ......
// It goes beyond the bounds of all other forms of professional +ing..
// Like ....., ..... is one of the most .... ....


// == Oracle log files

// While other data sources collected or transferred regularly, Oracle log files are collected only once. 
// CSC provides us log files which are available. 



// where are stored data
// what are input data at each chapter
// what we are doing with these data
// what are code parts ?
// where are stored output data












