<<<
== Nexthink

Nexthink is the application which collects information about any actions done on a PC, Laptop or a server.
See image below.
footnote:[https://doc.nexthink.com/images/a/a3/Collector.png]

image::images/collector.png[title="Nexthink Engine which collects data from different devices", width="400", height="300"]


There is a Nexthink _Collector_ which is installed on different devices.
It captures network connections, program executions, web requests, etc.. and sends data to the _Nexthink Engines_.
Nexthink Engines stock collected data and make a backup of them daily and per server. 


In fact, there are two groups of Nexthink Engine servers and collection of data is done separately on each of them.
There are 21 servers on the first group which work with v5 of Nexthink.
There are 3 servers in the second group.
The version of Nexthink is v6 in these servers.


Data collected from PCs, laptops, etc.. are stored on the first group of servers.
Whatever is collected from servers is saved on the second group of servers.

image::images/nexthink_collector_engins.jpg[title="Nexthink Engines which collect data to servers and backup them daily", width="300", height="350"]

Collected data are provided and stored under SAN server at /0/ folder level.
However, they need to be extracted and it must be done inside of a nexthink environment.
We have 2 VMs (nexthink v5 and v6) on the collector server which are dedicated to extract data to CSV format.

Once data are deployed correctly to these VMs and we use NXQL sql requests to extract information.
Extracted data are then stored on /1/ folder level.


Collected data are deployed on these VMs and then we extract information needed into CSV format.
Language used to extract information from collected data is NXQL.
And then we store them at level /1 folder.


Finally, extracted data is anonymized via VM20 and stored on s3://collect/nexthink/in.
Source code path: ALDatalab/collect/nexthink.


=== Data transfer from backup servers

Collector server (SARMA10012) collects backup data from Nexthink Collector servers every 3 days.
Each backup data can contain log history from 5 to 20 days.


Not all archived data is kept for a life.
The very old data are erased by the new once.
This is why we copy them every 3 days to SAN server in order to not loose a part of them.
At the same time, this causes us some overlaps that we will discuss about it later.


Data collection recovered from Nexthink environments are not in CSV format.
We need to read and extract them into CSV format and we need to realise this in a Nexthink environment.


There are 2 virtual machines on the collector server which are dedicated to extract information from "BackupData" to CSV format.
The first one is VM5 which is intended to extract data collected on the first group of servers.
The second one, VM6, is used to extract backup data coming from the second group of servers.


Data provided by Nexthink are kept on /0/ folder on SAN server.
They are dispatched either on VM5 or VM6 to be extracted in CSV format and then stored under /1/ folder on SAN server.
We call this operation _extractbackup_ which is executed daily.
See following image which describes these processes.

image::images/extract_anonymize.jpg[title="Nexthink backup extraction and data anonymization", width="450", height="250"]

After extracting, we separate 3 data types under /1/ folder.

* _connection_ - anything related to users' "connection" (TCP, UDP, etc..)

Information collected about any outgoing (and only outgoing) network requests,
such as which user is connected, by which application,
IP address requested, HTTP protocol used, server port number,
request execution time, request content size, etc.
These information are mainly related to the source device of the requests.
It can be a simple user machine but also a server.

* _webrequest_ - anything related to a "webrequest" (DNS information)

This kind of data are captured while a web request (HTTP) is detected.
Some information about the target device is collected, such as request's DNS address (google, etc.), etc.
However, full URL of web requests are not registered at all.


* _execution_ - anything related to an application "execution"

These are information about the execution of any application used by users.
This concern also applications which do not access to internet.
(even if a user doesn't login to the application).

These kind of data give us information about which application is executed,
by which user, at what time, the version of the application,
how much does it take to be started, the path to the application, etc.


As a result, there are 3 main folders ( _/connection_, /_webrequest_, /_execution_) under s3 server
for each of these types of data collections.


//[TIP]give a picture from cyberdock with highlighted colors of these folders.

=== Anonymization

The next step in the process is to make anonymous some user information from extracted data.
As usual, we use virtual machine VM20 for this purpose.
Once done, we store them under /2/ folder on the SAN server in CSV format.


Finally, these data is copied to _s3://gedatalab/in._

